{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "step1: 先讀取資料，再標準化所有feature values，觀察訓練與測試資料的大小。  \n",
    "step2: 完成myknn_regressor的class：暴力法的演算法是先算出每筆測試資料與各個訓練資料的距離，然後找出距離最近的k個鄰居，再依照不同類型算出Y_train的平均值。  \n",
    "step3: 計算實際值與預測值的RMSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 5000\n",
      "test_size: 3000\n"
     ]
    }
   ],
   "source": [
    "# size of data\n",
    "print(f'train_size: {X_train.shape[0]}')\n",
    "print(f'test_size: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  38.51784,   55.38723,   10.31652, ...,    6.66624,  -73.75585,\n",
       "           9.47857],\n",
       "       [  44.7309 ,   46.25776,    8.55636, ...,    4.5564 ,   53.49956,\n",
       "           5.39116],\n",
       "       [  43.08912,   45.70197,   17.18467, ...,  -13.64909,   -7.42137,\n",
       "          -4.97266],\n",
       "       ...,\n",
       "       [  39.81814,   62.12221,    6.60567, ...,   28.27807,  -60.00203,\n",
       "         -11.19226],\n",
       "       [  50.25968,   43.10325,   24.72866, ...,    2.71602,  -44.27226,\n",
       "          -2.54583],\n",
       "       [  41.39744,   -8.88692,  -20.15887, ...,    8.63798, -120.34024,\n",
       "           3.65709]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd_data['X_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myknn_regressor():\n",
    "    def __init__(self, n_neighbors = 10, mean_type = \"equal_weight\"):\n",
    "        \"\"\"mean_type can be equal_weight or remove_outliers.\n",
    "                              equal_weight use the same weight for all neighbors.\n",
    "                              remove_outliers remove neighbors out in [Q1 - 1.5 IQR, Q3 + 1.5IQR].\"\"\"\n",
    "        ### Add your code here ###  \n",
    "        '''save parameters'''   \n",
    "        self.n_neighbors = n_neighbors \n",
    "        self.mean_type = mean_type\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        ### Add your code here ###\n",
    "        '''save training data'''\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "         \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"use remove_outliers only if k>=10\"\"\"\n",
    "         ### Add your code here ###\n",
    "        if self.n_neighbors < 10:\n",
    "            self.mean_type = \"equal_weight\"\n",
    "        \n",
    "        \n",
    "        self.x_test = x_test\n",
    "        n_train = self.x_train.shape[0]\n",
    "        n_test = self.x_test.shape[0]\n",
    "        y_pred = []\n",
    "        \n",
    "        for i in range(n_test): # for every x_test\n",
    "            dist = []\n",
    "            for j in range(n_train): # for every x_train\n",
    "                # Euclidean distance between x_test's ith row and x_train's jth row\n",
    "                d = np.linalg.norm(self.x_test[i,:] - self.x_train[j,:])\n",
    "                dist.append([d, self.y_train[j]])\n",
    "            \n",
    "            sort_dist = sorted(dist, key = lambda l: l[0]) # sort y_train by distance in ascending order\n",
    "\n",
    "\n",
    "            # k nearest neighbors' y_train\n",
    "            k_neighbors = []\n",
    "            for k in range(self.n_neighbors):\n",
    "                k_neighbors.append(sort_dist[k][1]) \n",
    "\n",
    "            \n",
    "            if self.mean_type == \"equal_weight\":\n",
    "                y_pred.append(np.mean(k_neighbors))\n",
    "            else: # remove_outliers\n",
    "                q1 = np.quantile(k_neighbors,0.25) \n",
    "                q3 = np.quantile(k_neighbors,0.75)\n",
    "                iqr = q3 - q1\n",
    "                \n",
    "                new_neighbors = []\n",
    "                for k in range(len(k_neighbors)):\n",
    "                    y = k_neighbors[k]\n",
    "                    if y >= (q1 - (1.5 * iqr)) and y <= (q3 + (1.5 * iqr)):\n",
    "                        new_neighbors.append(y)\n",
    "\n",
    "                y_pred.append(np.mean(new_neighbors))\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use myknn_regressor\n",
    "myknn = myknn_regressor(20, \"equal_weight\")\n",
    "myknn.fit(X_train, Y_train)\n",
    "ypred = myknn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.25126451549596\n"
     ]
    }
   ],
   "source": [
    "# count RMSE\n",
    "actual = Y_test\n",
    "predicted = ypred\n",
    "square = np.square(np.subtract(actual, predicted))\n",
    "MSE = square.mean()\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "print(f'RMSE: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.2 | first 20 predictions:\n",
      "1993.35\n",
      "1993.8\n",
      "2000.65\n",
      "1991.5\n",
      "1992.8\n",
      "1998.5\n",
      "1988.1\n",
      "1991.65\n",
      "2002.25\n",
      "2003.0\n",
      "2000.5\n",
      "1998.65\n",
      "1995.55\n",
      "1997.2\n",
      "1995.05\n",
      "1997.4\n",
      "1992.15\n",
      "2000.45\n",
      "2003.2\n",
      "1995.75\n"
     ]
    }
   ],
   "source": [
    "# print first 20 predictions\n",
    "print(\"Q1.2 | first 20 predictions:\")\n",
    "for i in ypred[:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use myknn_regressor\n",
    "myknn = myknn_regressor(20, \"remove_outliers\")\n",
    "myknn.fit(X_train, Y_train)\n",
    "ypred = myknn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.212572466080376\n"
     ]
    }
   ],
   "source": [
    "# count RMSE\n",
    "actual = Y_test\n",
    "predicted = ypred\n",
    "square = np.square(np.subtract(actual, predicted))\n",
    "MSE = square.mean()\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "print(f'RMSE: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.3 | first 20 predictions:\n",
      "1993.35\n",
      "1993.8\n",
      "2000.65\n",
      "1992.7368421052631\n",
      "1992.8\n",
      "2000.0\n",
      "1988.1\n",
      "1991.65\n",
      "2002.25\n",
      "2003.9473684210527\n",
      "2000.5\n",
      "2000.9444444444443\n",
      "1995.55\n",
      "1997.2\n",
      "1998.611111111111\n",
      "1997.4\n",
      "1992.15\n",
      "2003.8333333333333\n",
      "2003.2\n",
      "1995.75\n"
     ]
    }
   ],
   "source": [
    "# print first 20 predictions\n",
    "print(\"Q1.3 | first 20 predictions:\")\n",
    "for i in ypred[:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "klist = [1 , 2 , 3 , 4 , 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 80 , 100 , 120 , 140 , 160 , 180 , 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list1 = []\n",
    "\n",
    "for k in klist:\n",
    "    knn1 = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn1.fit(X_train, Y_train)\n",
    "    ypred1 = knn1.predict(X_test)\n",
    "    \n",
    "    square1 = np.square(np.subtract(Y_test, ypred1))\n",
    "    mse1 = square1.mean()\n",
    "    rmse1 = np.sqrt(mse1)\n",
    "    rmse_list1.append(rmse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.911074724837043,\n",
       " 15.532793266720144,\n",
       " 16.940552922892845,\n",
       " 18.002388730387977,\n",
       " 18.343036826000212,\n",
       " 15.021429137513293,\n",
       " 13.296503299740124,\n",
       " 12.806235460378927,\n",
       " 12.530762147610975,\n",
       " 12.397271205121445,\n",
       " 12.46698306193871,\n",
       " 12.258833549730578,\n",
       " 12.234514021134364,\n",
       " 12.29800796877283,\n",
       " 12.29592073277421,\n",
       " 12.308465921199657,\n",
       " 12.539125434681107,\n",
       " 12.77006917235246,\n",
       " 12.875273459879082,\n",
       " 13.00256384974389,\n",
       " 13.039734148619237,\n",
       " 13.12649991429551,\n",
       " 13.190248923605145]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No feature scaling\n",
    "doscaling = 0\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list2 = []\n",
    "\n",
    "for k in klist:\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn2.fit(X_train, Y_train)\n",
    "    ypred2 = knn2.predict(X_test)\n",
    "    \n",
    "    square2 = np.square(np.subtract(Y_test, ypred2))\n",
    "    mse2 = square2.mean()\n",
    "    rmse2 = np.sqrt(mse2)\n",
    "    rmse_list2.append(rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.923638966418345,\n",
       " 16.527219971913002,\n",
       " 17.958025132699493,\n",
       " 18.922209173349714,\n",
       " 19.2830149786455,\n",
       " 15.532492824184189,\n",
       " 13.574596372145534,\n",
       " 13.365228517811932,\n",
       " 13.05419472813241,\n",
       " 12.818775292515271,\n",
       " 12.52252636917434,\n",
       " 12.618953997855765,\n",
       " 12.639356523705363,\n",
       " 12.519278466961797,\n",
       " 12.599576712466707,\n",
       " 12.60072749222573,\n",
       " 12.871764965743173,\n",
       " 13.047745654582123,\n",
       " 13.095648132108622,\n",
       " 13.170143000995344,\n",
       " 13.196754651554803,\n",
       " 13.26514229098203,\n",
       " 13.213793298418638]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No feature scaling\n",
    "doscaling = 1\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list3 = []\n",
    "\n",
    "for k in klist:\n",
    "    knn3 = myknn_regressor(k, \"remove_outliers\")\n",
    "    knn3.fit(X_train, Y_train)\n",
    "    ypred3 = np.array(knn3.predict(X_test))\n",
    "    \n",
    "    square3 = np.square(np.subtract(Y_test, ypred3))\n",
    "    mse3 = square3.mean()\n",
    "    rmse3 = np.sqrt(mse3)\n",
    "    rmse_list3.append(rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.911074724837043,\n",
       " 12.068170946750795,\n",
       " 11.389120864846157,\n",
       " 11.1123354880961,\n",
       " 10.894692897614565,\n",
       " 10.468549209697088,\n",
       " 10.293424167991525,\n",
       " 10.212572466080376,\n",
       " 10.19147537897133,\n",
       " 10.132105705172414,\n",
       " 10.109608004792985,\n",
       " 10.093231539122922,\n",
       " 10.080018891479627,\n",
       " 10.08078333324933,\n",
       " 10.059767600751963,\n",
       " 10.073857809066583,\n",
       " 10.063683549276588,\n",
       " 10.064917276737452,\n",
       " 10.067237203620246,\n",
       " 10.090646192390137,\n",
       " 10.115316675702928,\n",
       " 10.124987056716318,\n",
       " 10.146220720227745]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the error curve\n",
    "\n",
    "plt.plot(klist, rmse_list1, color='red', label='setting1')\n",
    "plt.plot(klist, rmse_list2, color='green', label='setting2')\n",
    "plt.plot(klist, rmse_list3, color='blue', label='setting3')\n",
    "plt.xlabel('k nearest neighbors') # title of x-axis\n",
    "plt.ylabel('RMSE') # tile of y-axis\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "1. setting1 vs setting2:  \n",
    "同樣是使用sklearn套件訓練出的模型，setting1不論k為多少，表現都比setting1還好。應該是因為特徵標準化的處理所致，k落在[25,50]時，RMSE的差距最大；若k小於25，RMSE的差距非常小，整體而言可發現特徵標準化對模型表現的改善。  \n",
    "2. setting1 vs setting3:\n",
    "這兩組設定的共同點都是有經過特徵標準化，很明顯地，暴力法的myknn表現遠比sklearn套件預設的方法更好。但暴力法的缺點就是時間複雜度較大，訓練、測試模型需要不少時間。若資料大小更大，模型ㄧ定會跑非常慢，暴力法可能就不太適合。\n",
    "3. setting3:\n",
    "從藍線可觀察出RMSE會隨k遞減，k=25之後就是趨近水平的曲線。直接觀察setting3得到的所有RMSE值就能得知55是最適k值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#Load data\n",
    "with open('msd_data1.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "#standardize feature values\n",
    "X_train_sd = xscaler.transform(msd_data['X_train'])\n",
    "X_test_sd = xscaler.transform(msd_data['X_test'])\n",
    "\n",
    "#outcome values\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./q3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tkinter import X\n",
    "import numpy as np\n",
    "\n",
    "class mylasso():\n",
    "    def __init__(self, lamcoef = 0.1, max_iter=1000, tol=1e-6, const_regu = False):\n",
    "        \"\"\"lamcoef: the regularization coefficient\n",
    "           max_iter: maximum number of iteration for model training\n",
    "           tol: tolerance for the stopping criteria for model training\n",
    "           const_regu: whether the constant term should be regularized, default to False\n",
    "           \"\"\"\n",
    "        ### Add your code here ###\n",
    "        self.lamcoef = lamcoef\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.const_regu = const_regu\n",
    "\n",
    "    def fit(self, x_train, y_train, winit = \"ridge\", keep_traindata = True, verbose = False):\n",
    "        \"\"\"Fit a Lasso model\n",
    "           x_train: a numpy matrix that contains training features\n",
    "           y_train: a numpy matrix that contains the label\n",
    "           winit: how weights are initialized, default to \"ridge\", ridge regression\n",
    "           keep_traindata: whether the object is going to keep training data after the training process completed\n",
    "           verbose: output a lot of message\"\"\"\n",
    "        ### Add your code here ###\n",
    "        if keep_traindata: # save training data\n",
    "            self.x_train = deepcopy(x_train)\n",
    "            self.y_train = deepcopy(y_train)\n",
    "\n",
    "        def soft_threshold(w, lam):\n",
    "            if w > 0.0 and lam < abs(w):\n",
    "                return w - lam\n",
    "            elif w < 0.0 and lam < abs(w):\n",
    "                return w + lam\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        \n",
    "        x_train = np.column_stack((np.ones(len(x_train)), x_train)) # add one column\n",
    "        W = np.zeros(x_train.shape[1]) # row vector\n",
    "        \n",
    "        # Initialize W: L2 regularization\n",
    "        # W = ((X^T)X)+ lambda*I)^(-1)(X^T)Y\n",
    "        identity = np.identity(x_train.shape[1])\n",
    "        X_T = np.transpose(x_train)\n",
    "        a = np.linalg.inv(np.dot(X_T, x_train) + self.lamcoef * identity)\n",
    "        b = np.dot(a, X_T)\n",
    "        W = np.dot(b, y_train)\n",
    "        # W[0] = np.sum(y_train - np.dot(x_train[:, 1:], W[1:])) / (x_train.shape[0]) # initial W\n",
    "        \n",
    "\n",
    "        L = 0.0\n",
    "        min_L = 1e+21\n",
    "        min_W = deepcopy(W)\n",
    "        for i in range(self.max_iter): \n",
    "            lastL = L\n",
    "            for j in range(1, len(W)):\n",
    "                tmp_W = deepcopy(W)\n",
    "                tmp_W[j] = 0.0\n",
    "                r_j = y_train - np.dot(x_train, tmp_W) #residual\n",
    "                arg1 = np.dot(x_train[:, j], r_j)\n",
    "                arg2 = self.lamcoef * (x_train.shape[0]) # lambda * n\n",
    "                W[j] = soft_threshold(arg1, arg2) / (x_train[:, j]**2).sum() # coordinate descent with soft thresholding\n",
    "                W[0] = np.sum(y_train - np.dot(x_train[:, 1:], W[1:])) / (x_train.shape[0])\n",
    "\n",
    "            W_T = np.transpose(W)\n",
    "            if self.const_regu: # L'\n",
    "                L = ((1 / 2 * x_train.shape[0]) * (((y_train - np.dot(x_train, W_T) - W[0])**2).sum())) + (self.lamcoef * ((abs(W).sum()) + abs(W[0])))\n",
    "            else: # L\n",
    "                L = ((1 / 2 * x_train.shape[0]) * (((y_train - np.dot(x_train, W_T) - W[0])**2).sum())) + (self.lamcoef * ((abs(W).sum())))\n",
    "\n",
    "            if L < min_L:\n",
    "                min_L = L\n",
    "                min_W = deepcopy(W)\n",
    "\n",
    "            if abs(L - lastL) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.loss = min_L\n",
    "        self.intercept = min_W[0]\n",
    "        self.coef = min_W[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        \"\"\"Make prediction using trained model\"\"\"\n",
    "        ### Add your code here ###\n",
    "        y_pred = np.dot(x_test, self.coef)\n",
    "        y_pred += self.intercept * np.ones(len(y_pred))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_sd\n",
    "y_train = Y_train\n",
    "x_test = X_test_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlo = mylasso(lamcoef = 0.1)\n",
    "mlo.fit(x_train, y_train)\n",
    "ypred = mlo.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.97706565, -2.71806405, -1.33697989,  0.34523198, -0.57712266,\n",
       "       -3.47877585,  0.        , -0.81707317, -0.40657806, -0.13621188,\n",
       "       -0.23391106, -0.3398293 ,  0.66632894,  0.        , -0.81557783,\n",
       "        0.62299946,  0.11253532,  0.96093136,  0.64542974,  0.69820802,\n",
       "        0.57481274,  0.43349445,  1.73978577,  0.20444774, -0.23991916,\n",
       "        0.07307756,  1.03034545,  0.04033037,  0.16071539,  0.        ,\n",
       "       -0.21937433, -0.25625984, -0.09803677, -0.03659762, -0.17931948,\n",
       "       -0.163607  ,  0.        ,  0.27274745,  0.41995879, -0.10427623,\n",
       "       -0.13900962, -0.2107629 , -0.02181362,  0.14584649, -0.20469432,\n",
       "        0.0759112 ,  0.03665381, -0.23554824,  0.        ,  0.12441931,\n",
       "        0.36848026,  0.        ,  0.07515772,  0.09085724,  0.        ,\n",
       "        0.02826299, -0.62450032,  0.48641559, -0.25204276,  0.08318992,\n",
       "       -0.1942295 ,  0.00052723, -0.17188622,  0.12940936, -0.554148  ,\n",
       "        0.        ,  0.11328508, -0.13782944, -0.10989864, -0.11030511,\n",
       "       -0.50475924,  0.0840557 ,  0.14543529,  0.19920287,  0.11874715,\n",
       "        0.15562786, -0.11485952, -0.63856932, -0.06328576,  0.        ,\n",
       "        0.09112546,  0.16583329,  0.26289277,  0.16699739,  0.41940858,\n",
       "        0.00252455,  0.        , -0.48688178, -0.00495036,  0.        ])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlo.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991.461722574251\n",
      "1998.3925469459427\n",
      "2003.3472824343007\n",
      "1990.4578107993975\n",
      "1994.3353046334\n"
     ]
    }
   ],
   "source": [
    "for i in ypred[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.653764842732576\n",
      "MAE: 6.934215776303553\n"
     ]
    }
   ],
   "source": [
    "# count RMSE and MAE\n",
    "square = np.square(np.subtract(Y_test, ypred))\n",
    "MSE = square.mean()\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "subtr = abs(np.subtract(Y_test, ypred))\n",
    "MAE = subtr.mean()\n",
    "\n",
    "print(f'RMSE: {RMSE}')\n",
    "print(f'MAE: {MAE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cfa4b7dcd7864583d72e5eda12e513f8c53c522d90114d3c5fb0ab3f428a316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
